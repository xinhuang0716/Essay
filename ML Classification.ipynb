{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22daf15b",
   "metadata": {},
   "source": [
    "<code>\n",
    "<br>\n",
    "Model List<br>\n",
    "<br/>\n",
    "</code>\n",
    "\n",
    "| Case | Dataset | Classification | Algorithm |\n",
    "| --- | --- | --- | --- |\n",
    "|1-1| Simple-Labeled | Multi-Classification (Single-Output) | SVM |\n",
    "|1-2| Simple-Labeled | Multi-Classification (Single-Output) | DT |\n",
    "|2-1| Simple-Labeled | One-Versus-Rest Binary-Classification | SVM |\n",
    "|2-2| Simple-Labeled | One-Versus-Rest Binary-Classification | DT |\n",
    "|3-1| Multi-Labeled | Multi-Classification (Multi-Output) | SVM |\n",
    "|3-2| Multi-Labeled | Multi-Classification (Multi-Output) | DT |\n",
    "|4-1| Multi-Labeled | One-Versus-Rest Binary-Classification | SVM |\n",
    "|4-2| Multi-Labeled | One-Versus-Rest Binary-Classification | DT |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a0b8b",
   "metadata": {},
   "source": [
    "- Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d783a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML package\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import re, string, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# evaluation matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# basic package\n",
    "import numpy as np, pandas as pd, random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f80583",
   "metadata": {},
   "source": [
    "- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3f722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DATA(single_label = True):\n",
    "    # read review datasets \n",
    "    def READ(game_name):\n",
    "        temp = pd.read_excel(f'data_cleaned\\\\labeled data\\\\{game_name}.xlsx')[:1000]\n",
    "        temp['game'] = game_name\n",
    "        if single_label:\n",
    "            return temp[temp.check == 1]\n",
    "        else:\n",
    "            return temp\n",
    "\n",
    "    # combine datasets\n",
    "    game_list = ['Warhammer 40,000 - Darktide','Brotato','Cult of the Lamb','Teardown','Mount & Blade 2 - Bannerlord']\n",
    "    df_train = pd.concat([READ(game) for game in game_list])\n",
    "    df_train = df_train[df_train.Spam == 0] # drop spam review\n",
    "    df_train.drop(columns = ['Spam', 'date', 'helpful_votes', 'check'], inplace = True) # drop unneccessary column\n",
    "    df_train.dropna(inplace = True)\n",
    "\n",
    "    # sampe \n",
    "    def SAMPLE(i, num, df = df_train, seed = SEED):\n",
    "        tag_list = ['Not helpful','Suggestion','Pro','Con','Bug']\n",
    "        df_temp = df[df[tag_list[i]]==1]\n",
    "        df_temp['Class'] = tag_list[i]\n",
    "        return df_temp[df_temp[tag_list[i]]==1].sample(n = num, random_state = SEED)\n",
    "    \n",
    "    if single_label:\n",
    "        df_train = pd.concat([SAMPLE(i, 140) for i in range(5)])\n",
    "    else:\n",
    "        df_train = pd.concat([df_train[df_train['Not helpful']!=1], SAMPLE(0, 250)])\n",
    "\n",
    "    # data preprocessing\n",
    "    sw = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text.lower()) # only keep (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        text = re.sub(r\"http\\S+\", \"\", text) #Removing URLs     \n",
    "        html = re.compile(r'<.*?>') \n",
    "        text = html.sub(r'', text) #Removing html tags\n",
    "        for punct in '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_':\n",
    "            text = text.replace(punct, '') #Removing punctuations\n",
    "        text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "        text = \" \".join([lemmatizer.lemmatize(word) for word in text]) #removing stopwords\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                            u\"\\U00002702-\\U000027B0\"\n",
    "                            u\"\\U000024C2-\\U0001F251\"\n",
    "                            \"]+\", flags = re.UNICODE) \n",
    "        return emoji_pattern.sub(r'', text) #Removing emojis  \n",
    "\n",
    "    df_train.review_clear = df_train.review_clear.apply(lambda x: clean_text(x))\n",
    "\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be192e04",
   "metadata": {},
   "source": [
    "- Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a14514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test \n",
    "def TRAIN_TEST(df):  \n",
    "    train_dataset, test_dataset = train_test_split(df, test_size = 0.3, random_state = SEED)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# vectorization\n",
    "def REPRESENTATION(df, train_dataset, test_dataset):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(df.review_clear.values.tolist())\n",
    "    X_train = vectorizer.transform(train_dataset.review_clear.values.tolist())\n",
    "    X_test = vectorizer.transform(test_dataset.review_clear.values.tolist())\n",
    "    return X_train, X_test\n",
    "\n",
    "def CLASSIFIER(X, Y, multi_output = False):\n",
    "    if multi_output == False:\n",
    "        DT, SVM = DecisionTreeClassifier(random_state = SEED), SVC(random_state = SEED)\n",
    "        DT.fit(X, Y), SVM.fit(X, Y)\n",
    "        return DT, SVM\n",
    "    else:\n",
    "        DT, SVM = DecisionTreeClassifier(random_state = SEED), MultiOutputClassifier(SVC(random_state = SEED))\n",
    "        DT.fit(X, Y), SVM.fit(X, Y)\n",
    "        return DT, SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486cf24",
   "metadata": {},
   "source": [
    "- Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9a812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the validation dataset\n",
    "def RESULT(y_pred, y_true, case = 0):\n",
    "    label_list = ['Not helpful','Suggestion','Pro','Con','Bug']\n",
    "    if case == 1:\n",
    "        print(classification_report(y_true ,y_pred))\n",
    "        print(np.transpose(confusion_matrix(y_true, y_pred, labels = label_list)))\n",
    "        for label in ['Not helpful','Suggestion','Pro','Con','Bug']:\n",
    "            y_true_01 = np.where(y_true == label, 1, 0)\n",
    "            y_pred_01 = np.where(y_pred == label, 1, 0)\n",
    "            print(f'the confusion matrix of {label}')\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true_01, y_pred_01).ravel()\n",
    "            print(np.array([[tp,fp],[fn,tn]]))\n",
    "        print('---'*15 + '\\n')\n",
    "    else:\n",
    "        print(f\"Number of prediction/real: {y_pred.sum()} / {y_true.sum()}\")\n",
    "        print(\"Accuracy:\", accuracy_score(y_pred, y_true))\n",
    "        print(\"Precision:\", precision_score(y_pred, y_true))\n",
    "        print(\"Recall:\", recall_score(y_pred, y_true))\n",
    "        print(\"F1 score:\", f1_score(y_pred, y_true))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        print('the confusion matrix:', np.array([[tp,fp],[fn,tn]]), sep = '\\n')\n",
    "        print('---'*15 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418aef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 9516)\n",
      "the Case4 DT confusion matrix of Suggestion\n",
      "Number of prediction/real: 60 / 56\n",
      "Accuracy: 0.893030794165316\n",
      "Precision: 0.44642857142857145\n",
      "Recall: 0.4166666666666667\n",
      "F1 score: 0.43103448275862066\n",
      "the confusion matrix:\n",
      "[[ 25  35]\n",
      " [ 31 526]]\n",
      "---------------------------------------------\n",
      "\n",
      "the Case4 SVM confusion matrix of Suggestion\n",
      "Number of prediction/real: 3 / 56\n",
      "Accuracy: 0.9108589951377634\n",
      "Precision: 0.03571428571428571\n",
      "Recall: 0.6666666666666666\n",
      "F1 score: 0.06779661016949153\n",
      "the confusion matrix:\n",
      "[[  2   1]\n",
      " [ 54 560]]\n",
      "---------------------------------------------\n",
      "\n",
      "(617, 9516)\n",
      "the Case4 DT confusion matrix of Pro\n",
      "Number of prediction/real: 278 / 303\n",
      "Accuracy: 0.6904376012965965\n",
      "Precision: 0.6435643564356436\n",
      "Recall: 0.7014388489208633\n",
      "F1 score: 0.6712564543889845\n",
      "the confusion matrix:\n",
      "[[195  83]\n",
      " [108 231]]\n",
      "---------------------------------------------\n",
      "\n",
      "the Case4 SVM confusion matrix of Pro\n",
      "Number of prediction/real: 278 / 303\n",
      "Accuracy: 0.8038897893030794\n",
      "Precision: 0.759075907590759\n",
      "Recall: 0.8273381294964028\n",
      "F1 score: 0.7917383820998279\n",
      "the confusion matrix:\n",
      "[[230  48]\n",
      " [ 73 266]]\n",
      "---------------------------------------------\n",
      "\n",
      "(617, 9516)\n",
      "the Case4 DT confusion matrix of Con\n",
      "Number of prediction/real: 198 / 209\n",
      "Accuracy: 0.7844408427876823\n",
      "Precision: 0.6555023923444976\n",
      "Recall: 0.6919191919191919\n",
      "F1 score: 0.6732186732186731\n",
      "the confusion matrix:\n",
      "[[137  61]\n",
      " [ 72 347]]\n",
      "---------------------------------------------\n",
      "\n",
      "the Case4 SVM confusion matrix of Con\n",
      "Number of prediction/real: 127 / 209\n",
      "Accuracy: 0.8249594813614263\n",
      "Precision: 0.5454545454545454\n",
      "Recall: 0.8976377952755905\n",
      "F1 score: 0.6785714285714285\n",
      "the confusion matrix:\n",
      "[[114  13]\n",
      " [ 95 395]]\n",
      "---------------------------------------------\n",
      "\n",
      "(617, 9516)\n",
      "the Case4 DT confusion matrix of Bug\n",
      "Number of prediction/real: 115 / 112\n",
      "Accuracy: 0.9108589951377634\n",
      "Precision: 0.7678571428571429\n",
      "Recall: 0.7478260869565218\n",
      "F1 score: 0.7577092511013217\n",
      "the confusion matrix:\n",
      "[[ 86  29]\n",
      " [ 26 476]]\n",
      "---------------------------------------------\n",
      "\n",
      "the Case4 SVM confusion matrix of Bug\n",
      "Number of prediction/real: 25 / 112\n",
      "Accuracy: 0.8589951377633711\n",
      "Precision: 0.22321428571428573\n",
      "Recall: 1.0\n",
      "F1 score: 0.36496350364963503\n",
      "the confusion matrix:\n",
      "[[ 25   0]\n",
      " [ 87 505]]\n",
      "---------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tag_list, case = ['Not helpful','Suggestion','Pro','Con','Bug'], 4\n",
    "\n",
    "    if case == 1: # single-label dataset, multi-class\n",
    "        # training\n",
    "        df_train = DATA(single_label = True)\n",
    "        train_dataset, test_dataset = TRAIN_TEST(df_train)\n",
    "        X_train, X_test = REPRESENTATION(df_train, train_dataset, test_dataset)\n",
    "        DT, SVM = CLASSIFIER(X_train, train_dataset.Class.values)\n",
    "        # result\n",
    "        print('---Case1 DT---\\n')\n",
    "        RESULT(DT.predict(X_test), test_dataset.Class.values, case = 1)\n",
    "        print('---Case1 SVM---\\n')\n",
    "        RESULT(SVM.predict(X_test), test_dataset.Class.values, case = 1)\n",
    "\n",
    "    elif case == 2: # single-label dataset, One-vs-Rest\n",
    "        df_train = DATA(single_label = True)\n",
    "        for i in range(1,5):\n",
    "            # training\n",
    "            train_dataset, test_dataset = TRAIN_TEST(df_train)\n",
    "            X_train, X_test = REPRESENTATION(df_train, train_dataset, test_dataset)\n",
    "            DT, SVM = CLASSIFIER(X_train, train_dataset[tag_list[i]].values)\n",
    "            # result\n",
    "            print(f'the Case2 DT confusion matrix of {tag_list[i]}')\n",
    "            RESULT(DT.predict(X_test), test_dataset[tag_list[i]].values)\n",
    "            print(f'the Case2 SVM confusion matrix of {tag_list[i]}')\n",
    "            RESULT(SVM.predict(X_test), test_dataset[tag_list[i]].values)\n",
    "\n",
    "\n",
    "    elif case == 3: # multi-label dataset, multi-class\n",
    "        df_train = DATA(single_label = False)\n",
    "        # training\n",
    "        train_dataset, test_dataset = TRAIN_TEST(df_train)\n",
    "        X_train, X_test = REPRESENTATION(df_train, train_dataset, test_dataset)\n",
    "        DT, SVM = CLASSIFIER(X_train, train_dataset[tag_list].values, multi_output = True)\n",
    "        # result\n",
    "        for i in range(0,5):\n",
    "            print(f'the Case3 DT confusion matrix of {tag_list[i]}')\n",
    "            RESULT(DT.predict(X_test)[:,2], test_dataset[tag_list[i]].values)\n",
    "            print(f'the Case3 SVM confusion matrix of {tag_list[i]}')\n",
    "            RESULT(SVM.predict(X_test)[:,2], test_dataset[tag_list[i]].values)\n",
    "    \n",
    "\n",
    "    elif case == 4: # multi-label dataset, One-vs-Rest\n",
    "        df_train = DATA(single_label = False)\n",
    "        for i in range(1,5):\n",
    "            # training\n",
    "            train_dataset, test_dataset = TRAIN_TEST(df_train)\n",
    "            X_train, X_test = REPRESENTATION(df_train, train_dataset, test_dataset)\n",
    "            print(X_test.shape)\n",
    "            DT, SVM = CLASSIFIER(X_train, train_dataset[tag_list[i]].values)\n",
    "            # result\n",
    "            print(f'the Case4 DT confusion matrix of {tag_list[i]}')\n",
    "            RESULT(DT.predict(X_test), test_dataset[tag_list[i]].values)\n",
    "            print(f'the Case4 SVM confusion matrix of {tag_list[i]}')\n",
    "            RESULT(SVM.predict(X_test), test_dataset[tag_list[i]].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "73eba8690cfbc73746c909001b3c14158df08c70569551f888a72447c65a5f1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
